# -*- coding: utf-8 -*-\n"""\nMoonstock ML Training Pipeline\nComplete training script for base success prediction\nBased on Blueprint: Option B - Training Script\n"""\n\nfrom __future__ import annotations\nimport pandas as pd\nimport numpy as np\nimport json\nimport joblib\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Dict, List, Tuple, Optional\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Scikit-learn\nfrom sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix, roc_auc_score, \n    roc_curve, precision_recall_curve, f1_score, accuracy_score\n)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\n\n# XGBoost and LightGBM\ntry:\n    from xgboost import XGBClassifier\n    HAS_XGB = True\nexcept ImportError:\n    HAS_XGB = False\n    print("‚ö†Ô∏è XGBoost not installed. Install with: pip install xgboost")\n\ntry:\n    from lightgbm import LGBMClassifier\n    HAS_LGBM = True\nexcept ImportError:\n    HAS_LGBM = False\n    print("‚ö†Ô∏è LightGBM not installed. Install with: pip install lightgbm")\n\n# Plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n\n# ====== Configuration ======\nclass TrainingConfig:\n    """Training configuration"""\n    \n    # Data filtering\n    MIN_LIQUIDITY = 200_000_000  # 200M IDR minimum ADV\n    EXCLUDE_RALLY_FLAG = True     # Exclude late entries\n    \n    # Train/test split\n    TEST_SIZE = 0.2\n    TEMPORAL_SPLIT = True         # Use temporal split instead of random\n    RANDOM_STATE = 42\n    \n    # Feature selection\n    FEATURE_GROUPS = {\n        'technical': [\n            'rvol_15', 'near_avwap_pct', 'rsi_14', 'bb_pct_b',\n            'ret_1d', 'ret_5d', 'ret_20d', 'volatility20',\n            'atr14_pct', 'donchian20_pos', 'ma5_gap', 'ma20_gap',\n            'rvol10', 'rvol20', 'hl_spread20_pct'\n        ],\n        'base_quality': [\n            'base_duration', 'base_range_pct', 'base_avg_rvol',\n            'base_volume_trend', 'base_days_above_avwap', 'base_days_near_avwap'\n        ],\n        'fundamental': [\n            'pe_ratio', 'pb_ratio', 'ps_ratio', 'profit_margin',\n            'roe', 'roa', 'revenue_growth', 'earnings_growth',\n            'debt_to_equity', 'current_ratio', 'quick_ratio',\n            'pe_vs_sector', 'pb_vs_sector'\n        ],\n        'advanced_technical': [\n            'macd', 'macd_signal', 'macd_histogram',\n            'adx_14', 'obv_ema20_ratio', 'mfi_14', 'cci_20'\n        ],\n        'market_context': [\n            'ihsg_ret_20d', 'ihsg_volatility20', 'ihsg_ma20_gap',\n            'correlation_to_ihsg'\n        ],\n        'liquidity': [\n            'ADV20_IDR', 'ADV60_IDR', 'MedValue20_IDR',\n            '%DaysValue‚â•Min_20D', 'Turnover20_%', 'Amihud20_(√ó1e-9)'\n        ]\n    }\n    \n    # Model parameters\n    MODELS = {\n        'logistic': {\n            'name': 'Logistic Regression',\n            'model': LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),\n            'enabled': True\n        },\n        'random_forest': {\n            'name': 'Random Forest',\n            'model': RandomForestClassifier(\n                n_estimators=200, max_depth=10, min_samples_split=20,\n                random_state=RANDOM_STATE, n_jobs=-1\n            ),\n            'enabled': True\n        },\n        'gradient_boosting': {\n            'name': 'Gradient Boosting',\n            'model': GradientBoostingClassifier(\n                n_estimators=200, learning_rate=0.1, max_depth=6,\n                random_state=RANDOM_STATE\n            ),\n            'enabled': True\n        },\n        'xgboost': {\n            'name': 'XGBoost',\n            'model': XGBClassifier(\n                n_estimators=200, learning_rate=0.1, max_depth=6,\n                random_state=RANDOM_STATE, use_label_encoder=False,\n                eval_metric='logloss'\n            ) if HAS_XGB else None,\n            'enabled': HAS_XGB\n        },\n        'lightgbm': {\n            'name': 'LightGBM',\n            'model': LGBMClassifier(\n                n_estimators=200, learning_rate=0.1, max_depth=6,\n                random_state=RANDOM_STATE, verbose=-1\n            ) if HAS_LGBM else None,\n            'enabled': HAS_LGBM\n        }\n    }\n    \n    # Calibration\n    CALIBRATION_METHOD = 'sigmoid'  # or 'isotonic'\n    CALIBRATION_CV = 5\n    \n    # Output\n    OUTPUT_DIR = Path('models')\n    PLOTS_DIR = Path('plots')\n\n# ====== Data Loading & Preprocessing ======\ndef load_and_filter_data(\n    events_file: str,\n    config: TrainingConfig\n) -> pd.DataFrame:\n    """\n    Load events_training.csv and apply quality filters\n    \n    Args:\n        events_file: Path to events_training.csv\n        config: Training configuration\n    \n    Returns:\n        Filtered DataFrame\n    """\n    print(f"\nüìÇ Loading data from {events_file}...")\n    df = pd.read_csv(events_file)\n    \n    initial_count = len(df)\n    print(f"   Initial events: {initial_count}")\n    \n    # Filter 1: Has label\n    df = df[df['dir_label'].notna()].copy()\n    print(f"   After label filter: {len(df)} ({len(df)/initial_count:.1%})")\n    \n    # Filter 2: Liquidity\n    if 'ADV20_IDR' in df.columns:\n        df = df[df['ADV20_IDR'] >= config.MIN_LIQUIDITY].copy()\n        print(f"   After liquidity filter (‚â•{config.MIN_LIQUIDITY:,.0f}): {len(df)} ({len(df)/initial_count:.1%})")\n    \n    # Filter 3: Rally flag (exclude late entries)\n    if config.EXCLUDE_RALLY_FLAG and 'rallyflag10' in df.columns:\n        before = len(df)\n        df = df[df['rallyflag10'] == False].copy()\n        print(f"   After rally flag filter: {len(df)} ({len(df)/initial_count:.1%})")\n    \n    # Convert date column\n    if 't0_date' in df.columns:\n        df['t0_date'] = pd.to_datetime(df['t0_date'])\n        df = df.sort_values('t0_date').reset_index(drop=True)\n    \n    print(f"\n‚úÖ Final dataset: {len(df)} events")\n    print(f"   Success rate: {(df['dir_label'] == 1).mean():.2%}")\n    print(f"   Date range: {df['t0_date'].min()} to {df['t0_date'].max()}")\n    \n    return df\n\ndef get_available_features(df: pd.DataFrame, config: TrainingConfig) -> List[str]:\n    """\n    Get list of available features from config\n    \n    Args:\n        df: Input DataFrame\n        config: Training configuration\n    \n    Returns:\n        List of feature column names\n    """\n    all_features = []\n    for group_name, features in config.FEATURE_GROUPS.items():\n        available = [f for f in features if f in df.columns]\n        all_features.extend(available)\n        \n        if available:\n            print(f"   {group_name}: {len(available)}/{len(features)} features")\n    \n    return all_features\n\ndef prepare_features(\n    df: pd.DataFrame,\n    feature_cols: List[str],\n    config: TrainingConfig\n) -> Tuple[pd.DataFrame, pd.Series, pd.Series]:\n    """\n    Prepare feature matrix and labels\n    \n    Args:\n        df: Input DataFrame\n        feature_cols: List of feature columns\n        config: Training configuration\n    \n    Returns:\n        X (features), y (labels), dates\n    """\n    print(f"\nüîß Preparing features...")\n    \n    # Extract features and labels\n    X = df[feature_cols].copy()\n    y = df['dir_label'].copy()\n    dates = df['t0_date'] if 't0_date' in df.columns else None\n    \n    print(f"   Features: {X.shape[1]} columns, {X.shape[0]} rows")\n    print(f"   Labels: {y.nunique()} classes")\n    \n    # Handle missing values\n    missing_count = X.isnull().sum().sum()\n    if missing_count > 0:\n        print(f"   ‚ö†Ô∏è Missing values: {missing_count}")\n        print("   Filling with column medians...")\n        X = X.fillna(X.median())\n    \n    # Check for infinite values\n    inf_count = np.isinf(X.values).sum()\n    if inf_count > 0:\n        print(f"   ‚ö†Ô∏è Infinite values: {inf_count}")\n        print("   Replacing with column max/min...")\n        X = X.replace([np.inf, -np.inf], np.nan)\n        X = X.fillna(X.median())\n    \n    print(f"‚úÖ Feature preparation complete")\n    \n    return X, y, dates\n\ndef temporal_train_test_split(\n    X: pd.DataFrame,\n    y: pd.Series,\n    dates: Optional[pd.Series],\n    test_size: float\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n    """\n    Split data temporally (earlier = train, later = test)\n    \n    Args:\n        X: Features\n        y: Labels\n        dates: Date column (optional)\n        test_size: Proportion for test set\n    \n    Returns:\n        X_train, X_test, y_train, y_test\n    """\n    split_idx = int(len(X) * (1 - test_size))\n    \n    X_train = X.iloc[:split_idx]\n    X_test = X.iloc[split_idx:]\n    y_train = y.iloc[:split_idx]\n    y_test = y.iloc[split_idx:]\n    \n    if dates is not None:\n        print(f"   Train period: {dates.iloc[0]} to {dates.iloc[split_idx-1]}")\n        print(f"   Test period: {dates.iloc[split_idx]} to {dates.iloc[-1]}")\n    \n    return X_train, X_test, y_train, y_test\n\n# ====== Model Training ======\ndef train_models(\n    X_train: pd.DataFrame,\n    X_test: pd.DataFrame,\n    y_train: pd.Series,\n    y_test: pd.Series,\n    config: TrainingConfig\n) -> Dict:\n    """\n    Train multiple models and compare performance\n    \n    Args:\n        X_train, X_test: Training and test features\n        y_train, y_test: Training and test labels\n        config: Training configuration\n    \n    Returns:\n        Dictionary of trained models and metrics\n    """\n    print(f"\nü§ñ Training models...")\n    \n    # Scale features\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    results = {}\n    \n    for model_key, model_config in config.MODELS.items():\n        if not model_config['enabled']:\n            continue\n        \n        name = model_config['name']\n        model = model_config['model']\n        \n        print(f"\n{'='*60}")\n        print(f"Training: {name}")\n        print(f"{'='*60}")\n        \n        # Train\n        model.fit(X_train_scaled, y_train)\n        \n        # Predictions\n        y_pred_train = model.predict(X_train_scaled)\n        y_pred_test = model.predict(X_test_scaled)\n        y_proba_train = model.predict_proba(X_train_scaled)[:, 1]\n        y_proba_test = model.predict_proba(X_test_scaled)[:, 1]\n        \n        # Metrics\n        train_metrics = calculate_metrics(y_train, y_pred_train, y_proba_train)\n        test_metrics = calculate_metrics(y_test, y_pred_test, y_proba_test)\n        \n        print(f"\nüìä Training Set:")\n        print_metrics(train_metrics)\n        \n        print(f"\nüìä Test Set:")\n        print_metrics(test_metrics)\n        \n        # Store results\n        results[model_key] = {\n            'name': name,\n            'model': model,\n            'scaler': scaler,\n            'train_metrics': train_metrics,\n            'test_metrics': test_metrics,\n            'y_pred_test': y_pred_test,\n            'y_proba_test': y_proba_test\n        }\n    \n    return results\n\ndef calculate_metrics(y_true, y_pred, y_proba) -> Dict:\n    """Calculate classification metrics"""\n    return {\n        'accuracy': accuracy_score(y_true, y_pred),\n        'roc_auc': roc_auc_score(y_true, y_proba),\n        'f1': f1_score(y_true, y_pred),\n        'confusion_matrix': confusion_matrix(y_true, y_pred),\n        'classification_report': classification_report(y_true, y_pred, output_dict=True)\n    }\n\ndef print_metrics(metrics: Dict):\n    """Pretty print metrics"""\n    print(f"   Accuracy: {metrics['accuracy']:.4f}")\n    print(f"   ROC-AUC:  {metrics['roc_auc']:.4f}")\n    print(f"   F1-Score: {metrics['f1']:.4f}")\n    \n    cm = metrics['confusion_matrix']\n    print(f"\n   Confusion Matrix:")\n    print(f"   TN={cm[0,0]:<4} FP={cm[0,1]:<4}")\n    print(f"   FN={cm[1,0]:<4} TP={cm[1,1]:<4}")\n\ndef select_best_model(results: Dict) -> Tuple[str, Dict]:\n    """Select best model based on test ROC-AUC"""\n    best_key = max(results, key=lambda k: results[k]['test_metrics']['roc_auc'])\n    best_result = results[best_key]\n    \n    print(f"\nüèÜ Best Model: {best_result['name']}")\n    print(f"   Test ROC-AUC: {best_result['test_metrics']['roc_auc']:.4f}")\n    \n    return best_key, best_result\n\n# ====== Feature Importance ======\ndef analyze_feature_importance(\n    model,\n    feature_cols: List[str],\n    top_n: int = 20\n) -> pd.DataFrame:\n    """Analyze and plot feature importance"""\n    print(f"\nüìä Feature Importance Analysis...")\n    \n    if hasattr(model, 'feature_importances_'):\n        importances = model.feature_importances_\n    elif hasattr(model, 'coef_'):\n        importances = np.abs(model.coef_[0])\n    else:\n        print("   ‚ö†Ô∏è Model does not support feature importance")\n        return pd.DataFrame()\n    \n    # Create DataFrame\n    importance_df = pd.DataFrame({\n        'feature': feature_cols,\n        'importance': importances\n    }).sort_values('importance', ascending=False)\n    \n    # Print top features\n    print(f"\nüîù Top {top_n} Features:")\n    for i, row in importance_df.head(top_n).iterrows():\n        print(f"   {row['feature']:30s} {row['importance']:.6f}")\n    \n    return importance_df\n\ndef plot_feature_importance(\n    importance_df: pd.DataFrame,\n    top_n: int,\n    output_path: Path\n):\n    """Plot feature importance"""\n    plt.figure(figsize=(10, 8))\n    \n    top_features = importance_df.head(top_n)\n    \n    plt.barh(range(len(top_features)), top_features['importance'])\n    plt.yticks(range(len(top_features)), top_features['feature'])\n    plt.xlabel('Importance')\n    plt.title(f'Top {top_n} Feature Importances')\n    plt.gca().invert_yaxis()\n    plt.tight_layout()\n    \n    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n    plt.close()\n    \n    print(f"   ‚úÖ Saved: {output_path}")\n\n# ====== Probability Calibration ======\ndef calibrate_model(\n    model,\n    X_train: np.ndarray,\n    y_train: pd.Series,\n    config: TrainingConfig\n) -> CalibratedClassifierCV:\n    """Calibrate model probabilities"""\n    print(f"\nüéØ Calibrating probabilities...")\n    print(f"   Method: {config.CALIBRATION_METHOD}")\n    print(f"   CV folds: {config.CALIBRATION_CV}")\n    \n    calibrated = CalibratedClassifierCV(\n        model,\n        method=config.CALIBRATION_METHOD,\n        cv=config.CALIBRATION_CV\n    )\n    \n    calibrated.fit(X_train, y_train)\n    \n    print(f"   ‚úÖ Calibration complete")\n    \n    return calibrated\n\ndef plot_calibration_curve(\n    y_test: pd.Series,\n    y_proba: np.ndarray,\n    output_path: Path\n):\n    """Plot probability calibration curve"""\n    # Bin predictions\n    bins = np.linspace(0, 1, 11)\n    bin_centers = (bins[:-1] + bins[1:]) / 2\n    bin_indices = np.digitize(y_proba, bins) - 1\n    \n    # Calculate actual success rate per bin\n    actual_rates = []\n    counts = []\n    \n    for i in range(10):\n        mask = (bin_indices == i)\n        if mask.sum() > 0:\n            actual_rates.append(y_test[mask].mean())\n            counts.append(mask.sum())\n        else:\n            actual_rates.append(np.nan)\n            counts.append(0)\n    \n    # Plot\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # Calibration curve\n    ax1.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration', linewidth=2)\n    ax1.plot(bin_centers, actual_rates, 'o-', label='Model', markersize=8, linewidth=2)\n    ax1.set_xlabel('Predicted Probability')\n    ax1.set_ylabel('Actual Success Rate')\n    ax1.set_title('Probability Calibration Curve')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # Distribution\n    ax2.hist(y_proba, bins=20, alpha=0.7, edgecolor='black')\n    ax2.set_xlabel('Predicted Probability')\n    ax2.set_ylabel('Count')\n    ax2.set_title('Prediction Distribution')\n    ax2.grid(True, alpha=0.3, axis='y')\n    \n    plt.tight_layout()\n    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n    plt.close()\n    \n    print(f"   ‚úÖ Saved: {output_path}")\n\n# ====== Threshold Optimization ======\ndef optimize_threshold(\n    y_test: pd.Series,\n    y_proba: np.ndarray\n) -> Dict:\n    """Find optimal classification threshold"""\n    print(f"\nüéØ Optimizing classification threshold...")\n    \n    # Precision-recall curve\n    precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n    \n    # F1 scores\n    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n    \n    # Optimal threshold (max F1)\n    optimal_idx = np.argmax(f1_scores)\n    optimal_threshold = thresholds[optimal_idx]\n    \n    print(f"\nüéØ Optimal Threshold (Max F1): {optimal_threshold:.3f}")\n    print(f"   Precision: {precision[optimal_idx]:.3f}")\n    print(f"   Recall:    {recall[optimal_idx]:.3f}")\n    print(f"   F1-Score:  {f1_scores[optimal_idx]:.3f}")\n    \n    # Conservative threshold (70% precision)\n    conservative_idx = np.where(precision >= 0.70)[0]\n    if len(conservative_idx) > 0:\n        conservative_idx = conservative_idx[0]\n        conservative_threshold = thresholds[conservative_idx]\n        \n        print(f"\nüõ°Ô∏è Conservative Threshold (70% Precision): {conservative_threshold:.3f}")\n        print(f"   Precision: {precision[conservative_idx]:.3f}")\n        print(f"   Recall:    {recall[conservative_idx]:.3f}")\n    else:\n        conservative_threshold = optimal_threshold\n        print(f"\n‚ö†Ô∏è Cannot achieve 70% precision, using optimal threshold")\n    \n    # Aggressive threshold (70% recall)\n    aggressive_idx = np.where(recall >= 0.70)[0]\n    if len(aggressive_idx) > 0:\n        aggressive_idx = aggressive_idx[-1]\n        aggressive_threshold = thresholds[aggressive_idx]\n        \n        print(f"\n‚ö° Aggressive Threshold (70% Recall): {aggressive_threshold:.3f}")\n        print(f"   Precision: {precision[aggressive_idx]:.3f}")\n        print(f"   Recall:    {recall[aggressive_idx]:.3f}")\n    else:\n        aggressive_threshold = optimal_threshold\n        print(f"\n‚ö†Ô∏è Cannot achieve 70% recall, using optimal threshold")\n    \n    return {\n        'optimal': {\n            'threshold': float(optimal_threshold),\n            'precision': float(precision[optimal_idx]),\n            'recall': float(recall[optimal_idx]),\n            'f1': float(f1_scores[optimal_idx])\n        },\n        'conservative': {\n            'threshold': float(conservative_threshold),\n            'precision': float(precision[conservative_idx]) if conservative_threshold != optimal_threshold else float(precision[optimal_idx]),\n            'recall': float(recall[conservative_idx]) if conservative_threshold != optimal_threshold else float(recall[optimal_idx])\n        },\n        'aggressive': {\n            'threshold': float(aggressive_threshold),\n            'precision': float(precision[aggressive_idx]) if aggressive_threshold != optimal_threshold else float(precision[optimal_idx]),\n            'recall': float(recall[aggressive_idx]) if aggressive_threshold != optimal_threshold else float(recall[optimal_idx])\n        },\n        'precision': precision.tolist(),\n        'recall': recall.tolist(),\n        'thresholds': thresholds.tolist()\n    }\n\ndef plot_threshold_analysis(\n    threshold_info: Dict,\n    output_path: Path\n):\n    """Plot precision-recall curve with thresholds"""\n    precision = np.array(threshold_info['precision'])\n    recall = np.array(threshold_info['recall'])\n    \n    plt.figure(figsize=(10, 6))\n    \n    plt.plot(recall, precision, linewidth=2, label='PR Curve')\n    \n    # Mark optimal\n    opt = threshold_info['optimal']\n    plt.scatter(opt['recall'], opt['precision'], s=200, c='red', \n                marker='*', zorder=5, label=f"Optimal (t={opt['threshold']:.3f})")\n    \n    # Mark conservative\n    cons = threshold_info['conservative']\n    plt.scatter(cons['recall'], cons['precision'], s=150, c='green', \n                marker='s', zorder=5, label=f"Conservative (t={cons['threshold']:.3f})")\n    \n    # Mark aggressive\n    agg = threshold_info['aggressive']\n    plt.scatter(agg['recall'], agg['precision'], s=150, c='orange', \n                marker='^', zorder=5, label=f"Aggressive (t={agg['threshold']:.3f})")\n    \n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curve with Optimal Thresholds')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    \n    plt.tight_layout()\n    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n    plt.close()\n    \n    print(f"   ‚úÖ Saved: {output_path}")\n\n# ====== Model Saving ======\ndef save_model(\n    calibrated_model,\n    scaler: StandardScaler,\n    feature_cols: List[str],\n    threshold_info: Dict,\n    train_info: Dict,\n    config: TrainingConfig\n) -> Path:\n    """Save trained model and metadata"""\n    print(f"\nüíæ Saving model...")\n    \n    # Create output directory\n    config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    \n    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")\n    \n    # Save model\n    model_path = config.OUTPUT_DIR / f"moonstock_model_{timestamp}.pkl"\n    joblib.dump(calibrated_model, model_path)\n    print(f"   ‚úÖ Model: {model_path}")\n    \n    # Save scaler\n    scaler_path = config.OUTPUT_DIR / f"moonstock_scaler_{timestamp}.pkl"\n    joblib.dump(scaler, scaler_path)\n    print(f"   ‚úÖ Scaler: {scaler_path}")\n    \n    # Save metadata\n    metadata = {\n        'model_name': train_info['best_model_name'],\n        'timestamp': timestamp,\n        'feature_cols': feature_cols,\n        'n_features': len(feature_cols),\n        'feature_groups': {k: [f for f in v if f in feature_cols] \n                          for k, v in config.FEATURE_GROUPS.items()},\n        'thresholds': {\n            'optimal': threshold_info['optimal']['threshold'],\n            'conservative': threshold_info['conservative']['threshold'],\n            'aggressive': threshold_info['aggressive']['threshold']\n        },\n        'performance': {\n            'train_roc_auc': train_info['train_roc_auc'],\n            'test_roc_auc': train_info['test_roc_auc'],\n            'test_f1': train_info['test_f1']\n        },\n        'training': {\n            'n_train': train_info['n_train'],\n            'n_test': train_info['n_test'],\n            'train_success_rate': train_info['train_success_rate'],\n            'test_success_rate': train_info['test_success_rate'],\n            'train_date_range': train_info.get('train_date_range'),\n            'test_date_range': train_info.get('test_date_range')\n        },\n        'label_config': {\n            'target_pct': 0.10,\n            'horizon_days': 20\n        },\n        'config': {\n            'min_liquidity': config.MIN_LIQUIDITY,\n            'exclude_rally_flag': config.EXCLUDE_RALLY_FLAG,\n            'temporal_split': config.TEMPORAL_SPLIT\n        }\n    }\n    \n    metadata_path = config.OUTPUT_DIR / f"moonstock_metadata_{timestamp}.json"\n    with open(metadata_path, 'w') as f:\n        json.dump(metadata, f, indent=2)\n    print(f"   ‚úÖ Metadata: {metadata_path}")\n    \n    # Copy to production names (without timestamp)\n    prod_model_path = config.OUTPUT_DIR / "moonstock_model.pkl"\n    prod_scaler_path = config.OUTPUT_DIR / "moonstock_scaler.pkl"\n    prod_metadata_path = config.OUTPUT_DIR / "moonstock_model_metadata.json"\n    \n    joblib.dump(calibrated_model, prod_model_path)\n    joblib.dump(scaler, prod_scaler_path)\n    with open(prod_metadata_path, 'w') as f:\n        json.dump(metadata, f, indent=2)\n    \n    print(f"\n   üì¶ Production files:")\n    print(f"      {prod_model_path}")\n    print(f"      {prod_scaler_path}")\n    print(f"      {prod_metadata_path}")\n    \n    return prod_model_path\n\n# ====== Plotting ======\ndef create_plots(\n    results: Dict,\n    best_key: str,\n    X_test: pd.DataFrame,\n    y_test: pd.Series,\n    importance_df: pd.DataFrame,\n    threshold_info: Dict,\n    config: TrainingConfig\n):\n    """Create all diagnostic plots"""\n    print(f"\nüìä Creating diagnostic plots...")\n    \n    config.PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n    \n    best_result = results[best_key]    \n    \n    # 1. Feature importance\n    if not importance_df.empty:\n        plot_feature_importance(\n            importance_df,\n            top_n=20,\n            output_path=config.PLOTS_DIR / "feature_importance.png"\n        )\n    \n    # 2. ROC curves (all models)\n    plot_roc_curves(results, config.PLOTS_DIR / "roc_curves.png")\n    \n    # 3. Calibration curve\n    scaler = best_result['scaler']\n    X_test_scaled = scaler.transform(X_test)\n    y_proba_calibrated = best_result['model'].predict_proba(X_test_scaled)[:, 1]\n    \n    plot_calibration_curve(\n        y_test,\n        y_proba_calibrated,\n        config.PLOTS_DIR / "calibration_curve.png"\n    )\n    \n    # 4. Threshold analysis\n    plot_threshold_analysis(\n        threshold_info,\n        config.PLOTS_DIR / "threshold_analysis.png"\n    )\n    \n    # 5. Confusion matrices\n    plot_confusion_matrices(\n        results,\n        config.PLOTS_DIR / "confusion_matrices.png"\n    )\n    \n    print(f"\n‚úÖ All plots saved to {config.PLOTS_DIR}/")\n\ndef plot_roc_curves(results: Dict, output_path: Path):\n    """Plot ROC curves for all models"""\n    plt.figure(figsize=(10, 8))\n    \n    for model_key, result in results.items():\n        y_test = result.get('y_test')\n        y_proba = result['y_proba_test']\n        \n        if y_test is None:\n            continue\n        \n        fpr, tpr, _ = roc_curve(y_test, y_proba)\n        auc = result['test_metrics']['roc_auc']\n        \n        plt.plot(fpr, tpr, linewidth=2, label=f"{result['name']} (AUC={auc:.3f})")\n    \n    plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curves - Model Comparison')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n    plt.close()\n    \n    print(f"   ‚úÖ Saved: {output_path}")\n\ndef plot_confusion_matrices(results: Dict, output_path: Path):\n    """Plot confusion matrices for all models"""\n    n_models = len(results)\n    fig, axes = plt.subplots(1, n_models, figsize=(5*n_models, 4))\n    \n    if n_models == 1:\n        axes = [axes]\n    \n    for ax, (model_key, result) in zip(axes, results.items()):\n        cm = result['test_metrics']['confusion_matrix']\n        \n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n                   xticklabels=['Fail', 'Success'],\n                   yticklabels=['Fail', 'Success'])\n        \n        ax.set_title(f"{result['name']}\n(AUC={result['test_metrics']['roc_auc']:.3f})")\n        ax.set_ylabel('True Label')\n        ax.set_xlabel('Predicted Label')\n    \n    plt.tight_layout()\n    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n    plt.close()\n    \n    print(f"   ‚úÖ Saved: {output_path}")\n\n# ====== Main Training Pipeline ======\ndef main(events_file: str = "events_training.csv"):\n    """Main training pipeline\n    \n    Args:\n        events_file: Path to events_training.csv\n    """\n    print("="*60)\n    print("üöÄ MOONSTOCK ML TRAINING PIPELINE")\n    print("="*60)\n    \n    config = TrainingConfig()\n    \n    # Step 1: Load and filter data\n    df = load_and_filter_data(events_file, config)\n    \n    if len(df) < 100:\n        print(f"\n‚ùå ERROR: Insufficient data ({len(df)} events). Need at least 100.")\n        return\n    \n    # Step 2: Prepare features\n    print(f"\nüìã Feature Groups:")\n    feature_cols = get_available_features(df, config)\n    \n    if len(feature_cols) == 0:\n        print(f"\n‚ùå ERROR: No features available!")\n        return\n    \n    X, y, dates = prepare_features(df, feature_cols, config)\n    \n    # Step 3: Train/test split\n    print(f"\n‚úÇÔ∏è Splitting data...")\n    if config.TEMPORAL_SPLIT and dates is not None:\n        print(f"   Using TEMPORAL split (test_size={config.TEST_SIZE})")\n        X_train, X_test, y_train, y_test = temporal_train_test_split(\n            X, y, dates, config.TEST_SIZE\n        )\n    else:\n        print(f"   Using RANDOM split (test_size={config.TEST_SIZE})")\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=config.TEST_SIZE, random_state=config.RANDOM_STATE,\n            stratify=y\n        )\n    \n    print(f"   Train: {len(X_train)} samples ({(y_train==1).mean():.2%} success)")\n    print(f"   Test:  {len(X_test)} samples ({(y_test==1).mean():.2%} success)")\n    \n    # Step 4: Train models\n    results = train_models(X_train, X_test, y_train, y_test, config)\n    \n    # Add y_test to results for plotting\n    for key in results:\n        results[key]['y_test'] = y_test\n    \n    # Step 5: Select best model\n    best_key, best_result = select_best_model(results)\n    \n    # Step 6: Feature importance\n    importance_df = analyze_feature_importance(\n        best_result['model'],\n        feature_cols,\n        top_n=20\n    )\n    \n    # Step 7: Calibrate best model\n    scaler = best_result['scaler']\n    X_train_scaled = scaler.transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    calibrated_model = calibrate_model(\n        best_result['model'],\n        X_train_scaled,\n        y_train,\n        config\n    )\n    \n    # Step 8: Optimize threshold\n    y_proba_calibrated = calibrated_model.predict_proba(X_test_scaled)[:, 1]\n    threshold_info = optimize_threshold(y_test, y_proba_calibrated)\n    \n    # Step 9: Create plots\n    create_plots(\n        results,\n        best_key,\n        X_test,\n        y_test,\n        importance_df,\n        threshold_info,\n        config\n    )\n    \n    # Step 10: Save model\n    train_info = {\n        'best_model_name': best_result['name'],\n        'train_roc_auc': best_result['train_metrics']['roc_auc'],\n        'test_roc_auc': best_result['test_metrics']['roc_auc'],\n        'test_f1': best_result['test_metrics']['f1'],\n        'n_train': len(X_train),\n        'n_test': len(X_test),\n        'train_success_rate': float((y_train == 1).mean()),\n        'test_success_rate': float((y_test == 1).mean())\n    }\n    \n    if dates is not None and config.TEMPORAL_SPLIT:\n        split_idx = int(len(dates) * (1 - config.TEST_SIZE))\n        train_info['train_date_range'] = [\n            str(dates.iloc[0]),\n            str(dates.iloc[split_idx-1])\n        ]\n        train_info['test_date_range'] = [\n            str(dates.iloc[split_idx]),\n            str(dates.iloc[-1])\n        ]\n    \n    model_path = save_model(\n        calibrated_model,\n        scaler,\n        feature_cols,\n        threshold_info,\n        train_info,\n        config\n    )\n    \n    # Final summary\n    print(f"\n" + "="*60)\n    print(f"‚úÖ TRAINING COMPLETE!")\n    print(f"="*60)\n    print(f"\nüì¶ Model: {model_path}")\n    print(f"üìä Test ROC-AUC: {train_info['test_roc_auc']:.4f}")\n    print(f"üéØ Recommended Threshold: {threshold_info['conservative']['threshold']:.3f}")\n    print(f"   (Precision: {threshold_info['conservative']['precision']:.3f}, ")\n    print(f"   Recall: {threshold_info['conservative']['recall']:.3f})")\n    print(f"\nüí° Next Steps:")\n    print(f"   1. Review plots in {config.PLOTS_DIR}/")\n    print(f"   2. Integrate model into moonstock_refactored.py")\n    print(f"   3. Run daily predictions!")\n    print()\n\nif __name__ == "__main__":\n    import sys\n    \n    if len(sys.argv) > 1:\n        events_file = sys.argv[1]\n    else:\n        events_file = "events_training.csv"\n    \n    try:\n        main(events_file)\n    except FileNotFoundError as e:\n        print(f"\n‚ùå ERROR: {e}")\n        print(f"\nüí° Usage: python train_model.py [events_training.csv]")\n        sys.exit(1)\n    except Exception as e:\n        print(f"\n‚ùå FATAL ERROR: {e}")\n        import traceback\n        traceback.print_exc()\n        sys.exit(1)\n